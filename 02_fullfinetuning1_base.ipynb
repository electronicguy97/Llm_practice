{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전체 미세조정(Full Fine-Tuning)\n",
    "\n",
    "참고 자료\n",
    "- [Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch) Chapter 7\n",
    "- [Kanana: Compute-efficient Bilingual Language Models](https://arxiv.org/abs/2502.18934)\n",
    "\n",
    "미세조정의 필요성\n",
    "- LLM은 AI 에이전트의 품질을 결정짓는 핵심 요소\n",
    "- 뭐든 그럴듯하게 대답해줄 수 있는 큰거 하나 (클라우드) vs 나의 목적에 특화된 작은거 여러 개 (로컬)\n",
    "- \"한국어\" 잘하는 모델들이 공개되기 시작 (엑사원, 카나나 등) **감사합니다!**\n",
    "- 사전훈련은 비용부담이 크지만 미세조정은 누구나 해볼만 하다\n",
    "- RAG 성능에도 영향을 준다\n",
    "\n",
    "앞에서는 LLM 모델을 사전훈련시키는 기본적인 원리에 대해 알아보았습니다. 사전훈련은 모델이 기본적인 언어 능력을 갖추도록 학습시키는 것으로 볼 수 있습니다. 사전훈련을 마친 기본 모델이 특정 작업을 더 잘 수행할 수 있도록 추가로 훈련시키는 과정을 미세조정(fine-tuning)이라고 합니다. \n",
    "\n",
    "LLM을 훈련시킬 때는 GPU 사용료가 큰 부담이 된다는 것은 널리 알려진 사실입니다. 다행스럽게도 미세조정을 잘 활용하면 훨씬 적은 비용으로 나의 특정 용도에 최적화된 모델을 만들 수 있습니다. 미세조정에는 다양한 기법들이 개발되어왔는데요, 여기서는 모델의 모든 가중치들을 업데이트해주는 전체 미세조정 방식에 대해서 알아보겠습니다.\n",
    "\n",
    "[안내]\n",
    "- 본 내용은 쉬운 이해를 돕기 위해 최소한의 예제를 바탕으로 작성되었습니다. 실제 적용 범위에 대한 오해가 없으시길 바랍니다.\n",
    "- 혹시 영상 업로드 후에 수정해야할 오류가 발견되면 강의노트에 적어두겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 준비\n",
    "\n",
    "여기에서는 [카카오 나노 2.1b 베이스 모델](https://huggingface.co/kakaocorp/kanana-nano-2.1b-base)을 사용하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from accelerate import Accelerator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"kakaocorp/kanana-nano-2.1b-base\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", #nf4\n",
    "    bnb_4bit_use_double_quant=True, #True\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map = 'auto',\n",
    "    quantization_config=bnb_config,\n",
    "    low_cpu_mem_usage = True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token # <|end_of_text|> 128001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_list = []\n",
    "with open(\"dycustomdata.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        qna = line.strip().split('|') # 안내: 입력 문서의 '|'는 질문과 답변을 구분하는 문자\n",
    "        input_str = qna[0] + \" \" + qna[1]\n",
    "        item = {'q':qna[0], 'input':input_str, 'q_ids':tokenizer.encode(qna[0]), 'input_ids':tokenizer.encode(input_str)}\n",
    "        qna_list.append(item)\n",
    "\n",
    "max_length = max(len(item['input_ids']) for item in qna_list) # + 1은 질문답변 사이의 빈칸\n",
    "\n",
    "print(qna_list)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파인튜닝 전에 어떻게 응답하는지 확인\n",
    "\n",
    "questions = [ qna['q'] for qna in qna_list]\n",
    "questions.append(\"너에 대해서 설명해봐.\")\n",
    "questions.append(\"이처럼 인간처럼 생각하고 행동하는 AI 모델은 \")\n",
    "questions.append(\"인공지능의 장점은\")\n",
    "questions.append(\"아인슈타인에 대해서 얘기해봐.\")\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    questions,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "# print(type(model))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=32,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "output_list = output.tolist()\n",
    "\n",
    "for i, output in enumerate(output_list):\n",
    "    print(f\"Q{i}: {tokenizer.decode(output, skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collate\n",
    "- [파이토치 CrossEntropy의 ignore index = -100](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOT = 128001 # instruct 모델과 다름\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, qna_list, max_length):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        for qa in qna_list:\n",
    "            token_ids = qa['input_ids']\n",
    "            input_chunk = token_ids\n",
    "            target_chunk = token_ids[1:]\n",
    "            input_chunk += [EOT]* (max_length - len(input_chunk))\n",
    "            target_chunk +=  [EOT]* (max_length - len(target_chunk))\n",
    "            len_ignore = len(qa['q_ids']) - 1 # target은 한 글자가 짧기 때문\n",
    "            target_chunk[:len_ignore] = [-100] * len_ignore\n",
    "\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "dataset = MyDataset(qna_list, max_length=max_length)\n",
    "train_loader = DataLoader(dataset, batch_size=1, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(i)\n",
    "\n",
    "y_temp = y[0].tolist()\n",
    "y_temp = [x for x in y_temp if x != -100] # -100은 제외하고 디코딩\n",
    "\n",
    "print(tokenizer.decode(x[0].tolist()))\n",
    "print(tokenizer.decode(y_temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 훈련\n",
    "\n",
    "[안내] 데이터셋이 너무 작아서 validation은 생략하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "#device = \"cpu\"\n",
    "torch.manual_seed(123)\n",
    "#model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001, weight_decay=0.01)\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장할 폴더 이름\n",
    "save_dir = \"baseModel\"\n",
    "os.makedirs(save_dir, exist_ok=True)  # 폴더가 없으면 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "tokens_seen, global_step = 0, -1\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for input_batch, target_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # GPU 자동 이동\n",
    "        inputs_device = next(model.parameters()).device\n",
    "        input_batch = input_batch.to(inputs_device)\n",
    "        target_batch = target_batch.to(inputs_device)\n",
    "\n",
    "        logits = model(input_batch).logits # .logits을 활용해 tensor만 가져옴\n",
    "        loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tokens_seen += input_batch.numel()\n",
    "        global_step += 1\n",
    "        #print(f\"{global_step} Tokens seen: {tokens_seen}\")\n",
    "        # tqdm에 현재 loss 값을 출력하도록 설정\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch: {epoch}, Loss: {avg_loss}\")\n",
    "\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 결과확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파인튜닝 후에 어떻게 응답하는지 확인\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    save_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # GPU에 자동으로 매핑\n",
    ")\n",
    "\n",
    "# 토크나이저 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [ qna['q'] for qna in qna_list]\n",
    "questions.append(\"아인슈타인이 매일하는 게임은?\")\n",
    "questions.append(\"아인슈타인에 대해서 얘기해봐.\")\n",
    "questions.append(\"카나나 모델에 대해서 설명해봐.\")\n",
    "questions.append(\"이처럼 인간처럼 생각하고 행동하는 AI 모델은 \")\n",
    "questions.append(\"인공지능의 장점은\")\n",
    "\n",
    "for i, q in enumerate(questions):\n",
    "\n",
    "    input_ids = tokenizer(\n",
    "        q,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "    # print(type(model))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=32,\n",
    "            attention_mask = (input_ids != 0).long(),\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False,\n",
    "            # temperature=1.2,\n",
    "            # top_k=5\n",
    "        )\n",
    "\n",
    "    output_list = output.tolist()\n",
    "\n",
    "    print(f\"Q{i}: {tokenizer.decode(output[0], skip_special_tokens=True)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\n",
    "    input(),\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "# print(type(model))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=32,\n",
    "        attention_mask = (input_ids != 0).long(),\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False,\n",
    "        # temperature=1.2,\n",
    "        # top_k=5\n",
    "    )\n",
    "\n",
    "output_list = output.tolist()\n",
    "\n",
    "print(f\"Q{i}: {tokenizer.decode(output[0], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기타\n",
    "\n",
    "허깅페이스 코드 참고한 부분들\n",
    "- [라마 모델](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)\n",
    "- [대답 생성하는 부분(generate)](https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py#L1906)\n",
    "- [실제로 모델을 사용하는 부분(forward)](https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py#L2827)\n",
    "- [훈련(train)](https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L2612)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
